{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing from libraries\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random\n",
    "import re\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "##Tokenizing and lower casing\n",
    "def tokenize(text):\n",
    "    A_tokens = []\n",
    "    tokens = parser(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.orth_.startswith('@'):\n",
    "            continue\n",
    "        else:\n",
    "            A_tokens.append(token.lower_)\n",
    "    return A_tokens\n",
    "\n",
    "\n",
    "##Lematization\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "for w in ['cat', 'saved', 'happy']:\n",
    "    print(w, get_lemma(w), get_lemma2(w))\n",
    "    \n",
    "\n",
    "##Downloading stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "\n",
    "##Tokenization specifications\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "##Tweets check\n",
    "text_data = []\n",
    "input_File = open(\"dataset/combinedtweet.txt\",\"r\",encoding = \"latin1\").read()\n",
    "tweet = input_File.split('\\n')\n",
    "document = []\n",
    "for t in tweet:\n",
    "      item = t.split('|')\n",
    "      if len(item) > = 3:\n",
    "          clean = re.sub(r\"http\\S+\", \"\", item[2])\n",
    "          document.append(clean)\n",
    "print (\"Cleaning is Done\")\n",
    "for line in document:\n",
    "    tokens = prepare_text_for_lda(line)\n",
    "    text_data.append(tokens)\n",
    "print (\"Done...\")\n",
    "\n",
    "len(text_data)\n",
    "\n",
    "##Importing corpora for dictionary \n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "\n",
    "##dictionary processing\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')\n",
    "\n",
    "###########\n",
    "##Clustering\n",
    "num_clust = 4\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_clust = num_clust, id2word=dictionary, passes=25)\n",
    "ldamodel.save('modelNew.gensim')\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    \n",
    "#Prediction class\n",
    "def predict_class(new_tweet):\n",
    "    new_tweet = prepare_text_for_lda(new_tweet)\n",
    "    new_tweet_bow = dictionary.doc2bow(new_tweet)\n",
    "    pred =ldamodel.get_document_topics(new_tweet_bow)\n",
    "\n",
    "    max=pred[0][1]\n",
    "    label=0\n",
    "    for i in range(1,4):\n",
    "        if (pred[i][1]>max):\n",
    "            max= pred[i][1]\n",
    "            label=i\n",
    "    return label\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('modelNew.gensim')\n",
    "\n",
    "\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.show(lda_display)\n",
    "\n",
    "lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "lda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display3)\n",
    "\n",
    "##Writing data\n",
    "csv_file_file = open(\"dataset/health_tweets_labeled.csv_file\", \"a\") \n",
    "columnTitleRow = \"tweet, class\\n\"\n",
    "csv_file.write(columnTitleRow)\n",
    "\n",
    "for i in range(13618,len(text_data)):\n",
    "    tweet = text_data[i]\n",
    "    tweet_str = ' '.join(str(e) for e in tweet)\n",
    "    row = tweet_str.replace(\",\",\"\") + \",\" + str(predict_class(tweet_str)) + \"\\n\"\n",
    "    csv_file.write(row)\n",
    "csv_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
